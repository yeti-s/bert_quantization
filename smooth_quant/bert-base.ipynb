{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"medium\" # tiny, mini, small, medium, base, large\n",
    "task_name = \"mnli\" # cola, mnli, qnli, qqp\n",
    "learning_rate = 2e-05\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = {\n",
    "    'tiny': 'google/bert_uncased_L-2_H-128_A-2',\n",
    "    'mini': 'google/bert_uncased_L-4_H-256_A-4',\n",
    "    'small': 'google/bert_uncased_L-4_H-512_A-8',\n",
    "    'medium': 'google/bert_uncased_L-8_H-512_A-8',\n",
    "    'base': 'google/bert_uncased_L-12_H-768_A-12',\n",
    "    'large': 'bert-large-uncased'\n",
    "}\n",
    "\n",
    "model_name = model_name[model_type]\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "task = {\n",
    "    \"qnli\":{\n",
    "        \"num_labels\": 2,\n",
    "        \"test_dataset_name\": \"validation\",\n",
    "        \"tokenize\": lambda data:tokenizer(data['question'], data['sentence'], truncation=True, max_length=max_length, padding='max_length')\n",
    "    },\n",
    "    \"mnli\":{\n",
    "        \"num_labels\": 3,\n",
    "        \"test_dataset_name\": \"validation_matched\",\n",
    "        \"tokenize\": lambda data:tokenizer(data['premise'], data['hypothesis'], truncation=True, max_length=max_length, padding='max_length')\n",
    "    },\n",
    "    \"qqp\":{\n",
    "        \"num_labels\": 2,\n",
    "        \"test_dataset_name\": \"validation\",\n",
    "        \"tokenize\": lambda data:tokenizer(data['question1'], data['question2'], truncation=True, max_length=max_length, padding='max_length')\n",
    "    },\n",
    "    \"cola\":{\n",
    "        \"num_labels\": 2,\n",
    "        \"test_dataset_name\": \"validation\",\n",
    "        \"tokenize\": lambda data:tokenizer(data['sentence'], truncation=True, max_length=max_length, padding='max_length')\n",
    "    }\n",
    "}\n",
    "\n",
    "task = task[task_name]\n",
    "saved_path = f'../ignore/task/bert-{model_type}_{task_name}.pt'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=task[\"num_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('glue', task_name)\n",
    "tokenize = task['tokenize']\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, batch_size = 5000)\n",
    "\n",
    "def create_dataloader(dataset, batch_size, random=False):\n",
    "    input_ids = torch.tensor(dataset['input_ids'])\n",
    "    attention_masks = torch.tensor(dataset['attention_mask'])\n",
    "    labels = torch.tensor(dataset['label'])\n",
    "    tensor_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    random_sampler = RandomSampler(tensor_dataset)\n",
    "    if random:\n",
    "        return DataLoader(tensor_dataset, batch_size = batch_size, sampler=random_sampler)\n",
    "    return DataLoader(tensor_dataset, batch_size = batch_size)\n",
    "\n",
    "num_validation = (len(tokenized_dataset[task['test_dataset_name']]) // 10)\n",
    "train_data_loader = create_dataloader(tokenized_dataset['train'], batch_size, random=True)\n",
    "validation_data_loader = create_dataloader(tokenized_dataset[task['test_dataset_name']][:num_validation], batch_size)\n",
    "test_data_loader = create_dataloader(tokenized_dataset[task['test_dataset_name']][num_validation:], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "sys.path.append(os.path.relpath(\"..\"))\n",
    "sys.path.append(os.path.relpath(\".\"))\n",
    "\n",
    "from utils import train_model, evaluate_model, draw_activation, draw_weight, replace_modules\n",
    "from smooth_quant import quantize_per_tensor_asymmetric, quantize_per_tensor_symmetric, get_act_scales, FakeQuantLinear, SmoothQuantLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(saved_path)\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "# train_model(model, train_data_loader, epochs=epochs, lr=learning_rate)\n",
    "# torch.save(model, saved_path)\n",
    "evaluate_model(model, test_data_loader, multiple_classes=task['num_labels'] > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_scales = get_act_scales(model.bert, validation_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "def smooth_quantize(model, method, better=False):\n",
    "    smooth_model = deepcopy(model)\n",
    "    \n",
    "    for name, act_scale in act_scales.items():\n",
    "        \n",
    "        \n",
    "        keys = name.split(\".\")\n",
    "        module = smooth_model.bert\n",
    "\n",
    "        for key in keys[:-1]:\n",
    "            module = getattr(module, key)\n",
    "        if 'attention.output.dense' in name:\n",
    "            setattr(module, keys[-1], SmoothQuantLinear(getattr(module, keys[-1]), act_scale, quantization_method=method))\n",
    "        elif better and ('output.dense' in name or 'intermediate.dense' in name):\n",
    "            setattr(module, keys[-1], FakeQuantLinear(getattr(module, keys[-1]), quantization_method=method))\n",
    "        else:\n",
    "            setattr(module, keys[-1], SmoothQuantLinear(getattr(module, keys[-1]), act_scale, quantization_method=method))\n",
    "        \n",
    "    return smooth_model\n",
    "\n",
    "\n",
    "def fake_quantize(model, method):\n",
    "    fq_model = deepcopy(model)\n",
    "    replace_modules(fq_model, nn.Linear, lambda model: FakeQuantLinear(model, quantization_method=method))\n",
    "    return fq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_model_sym = smooth_quantize(model, quantize_per_tensor_symmetric)\n",
    "smooth_model_asym = smooth_quantize(model, quantize_per_tensor_asymmetric)\n",
    "smooth_model_asym_better = smooth_quantize(model, quantize_per_tensor_asymmetric, True)\n",
    "fq_model_sym = fake_quantize(model, quantize_per_tensor_symmetric)\n",
    "fq_model_asym = fake_quantize(model, quantize_per_tensor_asymmetric)\n",
    "\n",
    "print(\"original model\")\n",
    "model.cuda()\n",
    "evaluate_model(model, test_data_loader, multiple_classes=task['num_labels'] > 2)\n",
    "\n",
    "print(\"smooth quantized model (asymmetric)\")\n",
    "smooth_model_asym.to('cuda')\n",
    "evaluate_model(smooth_model_asym, test_data_loader, multiple_classes=task['num_labels'] > 2)\n",
    "\n",
    "print(\"smooth quantized model (asymmetric better)\")\n",
    "smooth_model_asym_better.to('cuda')\n",
    "evaluate_model(smooth_model_asym_better, test_data_loader, multiple_classes=task['num_labels'] > 2)\n",
    "\n",
    "print(\"fake quantized model (asymmetric)\")\n",
    "fq_model_asym.to('cuda')\n",
    "evaluate_model(fq_model_asym, test_data_loader, multiple_classes=task['num_labels'] > 2)\n",
    "\n",
    "print(\"smooth quantized model (symmetric)\")\n",
    "smooth_model_sym.to('cuda')\n",
    "evaluate_model(smooth_model_sym, test_data_loader, multiple_classes=task['num_labels'] > 2)\n",
    "\n",
    "print(\"fake quantized model (symmetric)\")\n",
    "fq_model_sym.to('cuda')\n",
    "evaluate_model(fq_model_sym, test_data_loader, multiple_classes=task['num_labels'] > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_inputs(target_model, ref_model, data_loader):\n",
    "    target_model.to('cuda')\n",
    "    ref_model.to('cuda')\n",
    "    \n",
    "    diff_inputs = []\n",
    "    \n",
    "    for step, batch in enumerate(data_loader):\n",
    "        batch_inputs = tuple(t.to('cuda') for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch_inputs[0],\n",
    "            'attention_mask': batch_inputs[1],\n",
    "            'labels': batch_inputs[2]\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_outputs = target_model(**inputs)\n",
    "            ref_outputs = model(**inputs)\n",
    "            \n",
    "            target_predictions = target_outputs.logits.detach().cpu().numpy().argmax(axis=1)\n",
    "            ref_predictions = ref_outputs.logits.detach().cpu().numpy().argmax(axis=1)\n",
    "        \n",
    "            for idx in np.where(target_predictions != ref_predictions)[0]:\n",
    "                diff_inputs.append({\n",
    "                    'input_ids': batch_inputs[0][idx].unsqueeze(0),\n",
    "                    'attention_mask': batch_inputs[1][idx].unsqueeze(0)\n",
    "                })\n",
    "                \n",
    "    return diff_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_inputs = get_diff_inputs(smooth_model_asym, model, validation_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff_inputs[0]['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "model.to('cuda')\n",
    "error_threshold = 0.005\n",
    "hooks = []\n",
    "danger_ins = []\n",
    "danger_outs = []\n",
    "\n",
    "def mse_from_origin(m, x, y, name, origin_model):\n",
    "    loss = nn.functional.mse_loss(y[0], origin_model(x[0].squeeze(0)))\n",
    "    if loss > error_threshold:\n",
    "        print(name, loss)\n",
    "        danger_ins.append((name, x))\n",
    "        \n",
    "def register_hooks(model, origin_model, module_name):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, SmoothQuantLinear):\n",
    "            hooks.append(module.register_forward_hook(functools.partial(mse_from_origin, name=f'{module_name}.{name}', origin_model=getattr(origin_model, name))))\n",
    "        else:\n",
    "            register_hooks(module, getattr(origin_model, name), f'{module_name}.{name}')\n",
    "            \n",
    "register_hooks(smooth_model_asym.bert, model.bert, 'bert')\n",
    "\n",
    "smooth_model_asym.to('cuda')\n",
    "smooth_model_asym.eval()\n",
    "\n",
    "for input in diff_inputs:\n",
    "    with torch.no_grad():\n",
    "        smooth_model_asym(**input)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "num = 5\n",
    "\n",
    "for input in danger_ins:\n",
    "    print(input[0])\n",
    "    draw_activation(input[1][0])\n",
    "    i += 1\n",
    "    if i >= num:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
